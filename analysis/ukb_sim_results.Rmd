---
title: "Simulations with UK Biobank genotypes and different effect structure across traits"
author: "Fabio Morgante & Deborah Kunkel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r set options}
###Load libraries
library(ggplot2)
library(cowplot)

#repz <- 1:20
repz <- c(1:7,9:20)
prefix <- "output/prediction_accuracy/ukb_caucasian_white_british_unrel_100000"
metric <- "r2"
traitz <- 1:5
```

## Introduction

The goal of this analysis is to benchmark the newly developed *mr.mash.rss* (aka *mr.mash* with summary 
data) against already existing methods in the task of predicting phenotypes from genotypes using 
only summary data. To do so, we used real genotypes from the array data of the UK Biobank. We randomly 
sampled 105,000 nominally unrelated ($r_A$ < 0.025 between any pair) individuals of European ancestry 
(i.e., Caucasian and white British fields). After retaining variants with minor allele frequency 
(MAF) > 0.01, minor allele count (MAC) > 5, genotype missing rate < 0.1 and Hardy-Weinberg Equilibrium (HWE) 
test p-value > $1 *10^{-10}$, our data consisted of 595,071 genetic variants (i.e., our predictors). 
Missing genotypes were imputed with the mean genotype for the respective genetic variant.

The linkage disequilibrium (LD) matrices (i.e., the correlation matrices) were computed using 146,288 
nominally unrelated ($r_A$ < 0.025 between any pair) individuals of European ancestry (i.e., Caucasian 
and white British fields), that did not overlap with the 105,000 individuals used for the rest of the analyses.

For each replicate, we simulated 5 traits (i.e., our responses) by randomly sampling 5,000 variants 
(out of the total of 595,071) to be causal, with different effect sharing structures across traits (see below). 
The genetic effects explain 50\% of the total per-trait variance (except for two scenario as explained below)
-- in genetics terminology this is called genomic heritability ($h_g^2$). The residuals are uncorrelated across traits. 
Each trait was quantile normalized before all the analyses were performed.

We randomly sampled 5,000 (out of the 105,000) individuals to be the test set. The test set was only used to 
evaluate prediction accuracy. All the other steps were carried out on the training set of 100,000 individuals.

Summary statistics (i.e., effect size and its standard error) were obtained by univariate simple linear 
regression of each trait on each variant, one at a time. Variants were not standardized.

A few different methods were fitted:

* [LDpred2](https://academic.oup.com/bioinformatics/article/36/22-23/5424/6039173) per-chromosome with 
the auto option, 1000 iterations (after 500 burn-in iterations), $h^2$ initialized using an estimate from 
LD Score regression (LDSC) and $p$ initialized using the same grid as in the original paper. 
NB this is a univariate method.
<!-- * _LDpred2_ genome-wide with the auto option, 500 iterations (after 500 burn-in iterations), $h^2$  -->
<!-- initialized as 0.5/22 (WRONG) and $p$ initialized using the same grid as in the original paper. -->
<!-- NB this is a univariate method. -->
* _mr.mash.rss_ per-chromosome, with both canonical and data-driven covariance matrices computed 
as described in the [mvSuSiE paper](https://www.biorxiv.org/content/10.1101/2023.04.14.536893v1),
updating the (full rank) residual covariance and the mixture weights, without standardizing the variables. 
The residual covariance was initialized as in the [mvSuSiE paper](https://www.biorxiv.org/content/10.1101/2023.04.14.536893v1)
and the mixture weights were initialized as 90\% of the weight on the null component and 10\% of the
weight split equally across the remaining components. The phenotypic covariance was computed
as the sample covariance using the individual-level data. NB this is a multivariate method.
* [BayesR](https://www.nature.com/articles/s41467-019-12653-0) per-chromosome, with 5000 iterations, 
1000 burn-in iterations, thinning factor of 5, $\pi$ (i.e., the proportion of causal variants) 
initialized as 0.0001, $h^2$ initialized as 0.1, and other default parameters. NB this is a univariate method. 
We used the implementation in the *qgg* R package.
* [BayesC](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-186) per-chromosome, 
with 5000 iterations, 1000 burn-in iterations, thinning factor of 5, $\pi$ (i.e., the proportion of causal variants) 
initialized as 0.0001, $h^2$ initialized as 0.1, and other default parameters. NB this is a univariate method. 
We used the implementation in the *qgg* R package.
* [mvBayesC](https://academic.oup.com/genetics/article/209/1/89/5931015) per-chromosome, 
with 5000 iterations, 1000 burn-in iterations, thinning factor of 5, $\pi$ (i.e., the proportion of 
causal variants) initialized as 0.0001, $h^2$ initialized as 0.1,  and other default parameters. 
NB this is a multivariate method. We used the implementation in the *qgg* R package.
* _mvBayesCrest_ -- a version of _mvBayesC_ that only allows a variant to affect all or none of the traits -- 
per-chromosome, with 5000 iterations, 1000 burn-in iterations, thinning factor of 5, $\pi$ (i.e., the proportion of 
causal variants) initialized as 0.0001, $h^2$ initialized as 0.1,  and other default parameters. 
NB this is a multivariate method. We used the implementation in the *qgg* R package.

Prediction accuracy was evaluated as the $R^2$ of the regression of true phenotypes on the predicted 
phenotypes. This metric as the attractive property that its upper bound is $h_g^2$.

20 replicates for each simulation scenario were run.

## Equal effects scenario

In this scenario, the effects were drawn from a Multivariate Normal distribution with mean vector 0
and covariance matrix that achieves a per-trait variance of 1 and a correlation across traits of 1.
This implies that the effects of the causal variants are equal across responses.

```{r equal effects, fig.height=10, fig.width=13}
scenarioz <- "equal_effects_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_methods_shared <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0, 0.51) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "blue", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.5, linetype="dotted", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_methods_shared)
```

In this scenario, there is a clear advantage to using multivariate methods. In fact, given
that the effects are equal across traits and the residuals are uncorrelated, a multivariate
analysis is roughly equivalent to having 5 times as many samples as in an univariate
analysis. As expected, the results show that *mr.mash.rss* clearly does better than *LDpred2 auto*
*BayesC*, and *BayesR*, which perform similarly. *mvBayesC* (both versions) does better than the 
univariate methods, but not as well as *mr.mash.rss*.

## Mostly null scenario

In this scenario, the effects were drawn from a Multivariate Normal distribution with mean vector 0
and covariance matrix that achieves effects (with variance 1) to be present only trait 1. The other 
4 traits are only random noise.

```{r mostly null, fig.height=10, fig.width=13}
scenarioz <- "trait_1_only_effects_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_mostly_null <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0.0, 0.51) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "blue", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.5, linetype="dotted", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_mostly_null)
```

In this scenario, there is no advantage to using multivariate methods. In fact, there is potential
for multivariate methods to do worse than univariate methods because of the large amount of noise
modeled jointly with the signal. The results show that *mr.mash.rss* can learn this structure
from the data and performs similar to *LDpred2 auto* and *BayesR*, although a little worse.
*mvBayesC* performs slightly worse than *mr.mash.rss*. *mvBayesCrest* cannot adapt well to this scenario.

## Shared in subgroups scenario

In this scenario, the effects were drawn from a mixture of two Multivariate Normal distributions, i.e., 
$w_1 MVN(0, \Sigma_1) + w_2 MVN(0, \Sigma_2)$, where $w_1$ = 0.5 and $w_2$ = 0.5, $\Sigma_1$ is such 
that it achieves correlation across traits of 0.9 and variance of 1, $\Sigma_2$ is such that it achieves 
correlation across traits of 0.7 and variance of 1. The first component of the mixture applies to traits 1-3
while the second component applies to traits 4-5. The per-trait $h^2_g$ is 0.3 for traits 1-3 and 0.5 for the
traits 4-5.

```{r shared in subgroups, fig.height=10, fig.width=13}
scenarioz <- "blocks_shared_effects_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_blocks_shared <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0, 0.51) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "blue", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.5, linetype="dotted", linewidth=1, color = "black") +
  geom_hline(yintercept=0.3, linetype="dashed", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_blocks_shared)
```

In this scenario, multivariate methods can have some advantage over univariate methods, provided that
the former can adapt to the complex structure of the effects. The results show that all the methods perform
very similarly in traits 3 and 4 -- with *mvBayesC* being a tiny bit better -- but *mvBayesCrest*. 
However, *mr.mash.rss* and *mvBayesC* methods perform better than the univariate methods and *mvBayesCrest* 
in traits 1-3, due to the higher effect correlation across traits, the larger number of traits with shared 
effects, and the smaller $h^2_g$ (harder scenario for univariate methods).

## Equal effects scenario -- low PVE

In this scenario, the effects were drawn from a Multivariate Normal distribution with mean vector 0
and covariance matrix that achieves a per-trait variance of 1 and a correlation across traits of 1.
This implies that the effects of the causal variants are equal across responses. In addition, the 
per-trait genomic heritability was set to 0.2. In this scenario, $h^2$ was initialized as 0.01 in 
*BayesC*, *BayesR*, *mvBayesC* and *mvBayesCrest*.

```{r equal effects low pve, fig.height=10, fig.width=13}
scenarioz <- "equal_effects_low_pve_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_methods_shared_lowpve <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0, 0.2) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "blue", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.2, linetype="dotted", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_methods_shared_lowpve)
```

In this scenario, we expect the relative improvement of multivariate methods compared to univariate
methods to be larger than with $h^2_g = 0.5$. This is because with smaller signal-to-noise ratio,
it is harder for univariate methods to estimate effects accurately. Multivariate methods can borrow
information across traits (if effects are shared) and improve accuracy. The results show that 
the multivariate methods outperform the univariate methods, with *mr.mash.rss* being the best overall. 

## Equal effects scenario -- more polygenic

In this scenario, the effects were drawn from a Multivariate Normal distribution with mean vector 0
and covariance matrix that achieves a per-trait variance of 1 and a correlation across traits of 1.
This implies that the effects of the causal variants are equal across responses. In addition, the 
number of causal variants was set to 50,000. In this scenario, $\pi$ was initialized as 0.01 in 
*BayesC*, *BayesR*, *mvBayesC*, and *mvBayesCrest*.

```{r equal effects poly filt, fig.height=10, fig.width=13}
scenarioz <- "equal_effects_50000causal_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_methods_shared_lowpve <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0, 0.5) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "blue", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.5, linetype="dotted", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_methods_shared_lowpve)
```

In this scenario, we expect the accuracy to be lower because of the much larger number of causal
variants, each explaining a much lower proportion of the total $h^2_g=0.5$. Multivariate methods
can borrow information across traits (if effects are shared) and improve accuracy. The results 
show that *mr.mash.rss* clearly does better than the univariate methods in this scenario too.
*mvBayesC* (both versions) seems to have difficulties in this scenario.


## Equal effects scenario -- 10 traits

In this scenario, the effects were drawn from a Multivariate Normal distribution with mean vector 0
and covariance matrix that achieves a per-trait variance of 1 and a correlation across traits of 1.
This implies that the effects of the causal variants are equal across responses. In addition, the 
number of traits was increased to 10.

```{r equal effects 10 traits, fig.height=10, fig.width=13}
scenarioz <- "equal_effects_10traits_indep_resid"
methodz <- c("mr_mash_rss", "mvbayesC_rest", "ldpred2_auto", "bayesC", "bayesR")

i <- 0

traitz <- 1:10
repz <- c(1:10, 12:20)

n_col <- 6
n_row <- length(repz) * length(scenarioz) * length(methodz) * length(traitz)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scenario", "method", "trait", "metric", "score")

for(sce in scenarioz){
  for(met in methodz){
    for(repp in repz){
      dat <- readRDS(paste0(prefix, "_", sce, "_", met, "_pred_acc_", repp, ".rds"))
      
      for(trait in traitz){
        i <- i + 1
        
        res[i, 1] <- repp
        res[i, 2] <- sce
        res[i, 3] <- met
        res[i, 4] <- trait
        res[i, 5] <- metric
        res[i, 6] <- dat$r2[trait]
      }
    }
  }
}

res <- transform(res, scenario=as.factor(scenario),
                      method=as.factor(method),
                      trait=as.factor(trait))

p_methods_shared_10traits <- ggplot(res, aes(x = trait, y = score, fill = method)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  ylim(0, 0.51) +
  scale_fill_manual(values = c("pink", "red", "yellow", "green", "lightblue")) +
  labs(x = "Trait", y = expression(italic(R)^2), fill="Method", title="") +
  geom_hline(yintercept=0.5, linetype="dotted", linewidth=1, color = "black") +
  theme_cowplot(font_size = 18)

print(p_methods_shared_10traits)
```

In this scenario, we expect the relative improvement of multivariate methods compared to univariate
methods to be a little larger than with 5 traits. This is because multivariate methods can borrow
information across a larger number of traits (if effects are shared) and improve accuracy. The results 
show that *mr.mash.rss* clearly does better than the univariate methods, but the improvement compared 
to the scenario with 5 traits is small. *mvBayesCrest* does better than the univariate methods but worse 
than *mr.mash.rss*. *mvBayesC* is currently too slow to be run in this scenario. 